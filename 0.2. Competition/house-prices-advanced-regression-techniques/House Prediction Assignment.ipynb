{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "House Prediction Assignment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "communist-subcommittee"
      },
      "source": [
        "## House Price Prediction Assignment\n",
        "\n",
        "### Problem Statement:\n",
        "\n",
        "A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. \n",
        "\n",
        "We are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n",
        "\n",
        "\n",
        "### Business Goals:\n",
        "\n",
        "The company wants to know:\n",
        "1. Which variables are significant in predicting the price of a house, and\n",
        "2. How well those variables describe the price of a house.\n",
        "\n",
        "Also, determine the optimal value of lambda for ridge and lasso regression.\n",
        "\n",
        "\n",
        "### Steps:\n",
        "\n",
        "#### 1. Data Sourcing\n",
        "\n",
        "    1. Checking the encoding of the file\n",
        "    2. Loading the data\t\t\n",
        "\n",
        "#### 2. Data Exploring & Cleaning\n",
        "\n",
        "    A. Null Values Analysis\n",
        "        1. Identify and drop columns with 100% missing data\n",
        "        2. Identify and drop columns with more than 80% missing data\n",
        "        3. Identify and drop columns having single unique values as they will not any value to the analysis\n",
        "        4. Identify and drop unnecessary columns (like text based, Applicant Loan Behaviour)\n",
        "\n",
        "    B. Datatype Check \n",
        "\n",
        "    C. Datatype Conversion\n",
        "        1. Converting int to object\n",
        "    \n",
        "    D. Drop Records\n",
        "        1. Drop Duplicates\n",
        "    \n",
        "    E. Impute Null Values\n",
        "    \n",
        "    F. Populating the categorical columns with correct mapping\n",
        "    \n",
        "    G. Outliers handling\n",
        "    \n",
        "    G. Derived Metrics\n",
        "    \n",
        "#### 3. Data Visualisation\n",
        "#### 4. Data Preparation\n",
        "#### 5. Splitting and Scaling the data\n",
        "#### 6. Model Building & Evaluation\n",
        "#### 7. Regualisation using Ridge and Lasso\n",
        "#### 8. Making Predictions Using the Final Model on the test data\n",
        "######################################################################"
      ],
      "id": "communist-subcommittee"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "minute-psychiatry",
        "outputId": "0d0359d1-e0aa-41b5-dc20-13fbe76449da"
      },
      "source": [
        "# Importing all required packages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import cufflinks as cf\n",
        "import plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import ipywidgets as widgets\n",
        "from pandas.api.types import is_object_dtype,is_string_dtype, is_numeric_dtype\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "%matplotlib inline\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler,RobustScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.feature_selection import RFE,RFECV\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn import tree\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import os\n",
        "\n",
        "# hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "pd.set_option('display.max_columns', 300)\n",
        "pd.set_option('display.max_rows', 10000)\n",
        "py.offline.init_notebook_mode(connected=True) # plotting in offilne mode \n",
        "cf.set_config_file(offline=False, world_readable=True, theme='ggplot')\n",
        "pd.set_option('display.max_colwidth', 1) # make sure data and columns are displayed correctly withput purge\n",
        "pd.options.display.float_format = '{:20,.2f}'.format # display float value with correct precision "
      ],
      "id": "minute-psychiatry",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biblical-manchester"
      },
      "source": [
        "# Generic Functions\n",
        "\n",
        "# Method to get Meta-Data about any dataframe passed \n",
        "def getMetadata(dataframe) :\n",
        "    metadata_matrix = pd.DataFrame({\n",
        "                    'Datatype' : dataframe.dtypes, # data types of columns\n",
        "                    'Total_Element': dataframe.count(), # total elements in columns\n",
        "                    'Null_Count': dataframe.isnull().sum(), # total null values in columns\n",
        "                    'Null_Percentage': round(dataframe.isnull().sum()/len(dataframe) * 100,2) ,# percentage of null values\n",
        "                    'Unique_Value': dataframe.nunique()\n",
        "                       })\n",
        "    return metadata_matrix\n",
        "\n",
        "def getVIF(X):\n",
        "    vif = pd.DataFrame()\n",
        "    vif['Features'] = X.columns\n",
        "    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "    vif['VIF'] = round(vif['VIF'], 2)\n",
        "    vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
        "    return vif\n",
        "\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, \"no\": 0})\n",
        "\n",
        "def cross_validation(X_train,y_train,lm):\n",
        "    folds = KFold(n_splits = 2, shuffle = True, random_state = 100)\n",
        "    hyper_params = [{'n_features_to_select': list(range(len(X_train.columns)))}]\n",
        "    lm.fit(X_train, y_train)\n",
        "    rfe = RFE(lm)             \n",
        "    model_cv = GridSearchCV(estimator = rfe, \n",
        "                            param_grid = hyper_params, \n",
        "                            scoring= 'r2', \n",
        "                            cv = folds, \n",
        "                            verbose = 1,\n",
        "                            return_train_score=True)      \n",
        "\n",
        "    model_cv.fit(X_train, y_train)                  \n",
        "    cv_results = pd.DataFrame(model_cv.cv_results_)\n",
        "    plt.figure(figsize=(16,6))\n",
        "    plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
        "    plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
        "    plt.xlabel('number of features')\n",
        "    plt.ylabel('r-squared')\n",
        "    plt.title(\"Optimal Number of Features\")\n",
        "    plt.legend(['test score', 'train score'], loc='upper left')\n",
        "    \n",
        "def plot_bar_chart(plotting_frame,x_column,y_column) :\n",
        "            \n",
        "        x_axis_title = x_column.title()\n",
        "        y_axis_title = y_column.title()\n",
        "        \n",
        "        graph_title = \"Bar Chart [\" + x_axis_title.title() + \" Vs \" + y_axis_title.title() + \"]\"\n",
        "        \n",
        "        layout = go.Layout(\n",
        "             title = graph_title,\n",
        "             yaxis=dict(\n",
        "                title=y_axis_title\n",
        "             ),\n",
        "             xaxis=dict(\n",
        "                 title=x_axis_title\n",
        "             )\n",
        "        )\n",
        "\n",
        "        data_to_be_plotted = [\n",
        "            go.Bar(\n",
        "                x=plotting_frame[x_column], \n",
        "                y=plotting_frame[y_column]\n",
        "            )\n",
        "        ]\n",
        "\n",
        "\n",
        "        figure = go.Figure(data=data_to_be_plotted,layout=layout)\n",
        "        py.offline.iplot(figure)\n",
        "        \n",
        "        \n",
        "def plot_pie_chart(plotting_frame,x_column,y_column) : \n",
        "        \n",
        "        labels = plotting_frame[x_column].tolist()\n",
        "        values = plotting_frame[y_column].tolist()\n",
        "\n",
        "        trace = go.Pie(labels=labels, values=values)\n",
        "\n",
        "        py.offline.iplot([trace])\n",
        "\n",
        "        \n",
        "def plot_box_chart(dataframe) :\n",
        "    data = []\n",
        "    for index, column_name in enumerate(dataframe) :\n",
        "        data.append(\n",
        "        go.Box(\n",
        "            y=dataframe.iloc[:, index],\n",
        "            name=column_name\n",
        "         ))   \n",
        "        \n",
        "    layout = go.Layout(\n",
        "    yaxis=dict(\n",
        "        title=\"Frequency\",\n",
        "        zeroline=False\n",
        "    ),\n",
        "       boxmode='group'\n",
        "    )\n",
        "    \n",
        "    fig = go.Figure(data=data, layout=layout)    \n",
        "    py.offline.iplot(fig) \n",
        "    \n",
        "def plot_group_bar_chart(plot,col,hue) : \n",
        "    hue_col = pd.Series(data = hue)\n",
        "    fig, ax = plt.subplots()\n",
        "    width = len(plot[col].unique()) + 6 + 5*len(hue_col.unique())\n",
        "    fig.set_size_inches(width , 10)\n",
        "    ax = sns.countplot(data = loan_plot, x= col, order=plot[col].value_counts().index,hue = hue,palette=\"Set2\") \n",
        "    \n",
        "    for p in ax.patches:\n",
        "                # Some segment wise value we are getting as Nan as respective value not present to tackle the Nan using temp_height\n",
        "                temp_height = p.get_height()\n",
        "                \n",
        "                if math.isnan(temp_height):\n",
        "                    temp_height = 0.01\n",
        "                    \n",
        "                \n",
        "                ax.annotate('{:1.1f}%'.format((temp_height*100)/float(len(loan_plot))), (p.get_x()+0.05, temp_height+20)) \n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def col_list(df):\n",
        "    num_list = []\n",
        "    cat_list = []\n",
        "    for column in df:\n",
        "        if is_numeric_dtype(df[column]):\n",
        "            num_list.append(column)\n",
        "        elif is_object_dtype(df[column]):\n",
        "            cat_list.append(column)    \n",
        "    return cat_list,num_list\n",
        "\n",
        "def outliers(df,num_list):\n",
        "    oc = []\n",
        "    noc=[]\n",
        "    l=[]\n",
        "    u=[]\n",
        "    for c in num_list:\n",
        "        data=df[c].values\n",
        "        lower, upper = np.mean(data) - (np.std(data) * 3), np.mean(data) + (np.std(data) * 3)\n",
        "        outliers=len([x for x in data if x < lower or x > upper])\n",
        "        non_outliers=len([x for x in data if x >= lower and x <= upper])\n",
        "        l.append(lower)\n",
        "        u.append(upper)\n",
        "        oc.append(outliers)\n",
        "        noc.append(non_outliers)\n",
        "    oc_metric = pd.Series(oc, name = 'Outliers')\n",
        "    noc_metric = pd.Series(noc, name = 'Non-Outliers')\n",
        "    lower_limit = pd.Series(l, name = 'Lower Limit')\n",
        "    uper_limit = pd.Series(u, name = 'Upper Limit')    \n",
        "    outl = pd.DataFrame(num_list,columns = ['Columns'])\n",
        "    final_metric = pd.concat([outl, oc_metric, noc_metric,lower_limit,uper_limit], axis = 1)\n",
        " #   final_metric.set_index(\"Columns\", inplace = True)\n",
        "    return final_metric    \n",
        "\n",
        "def assumption_graph(y_train,y_pred_train):\n",
        "    \n",
        "    ### Assumption of Error Terms Being Independent\n",
        "    y_res_train = y_train - y_pred_train\n",
        "    plt.scatter( y_pred_train , y_res_train)\n",
        "    plt.axhline(y=0, color='r', linestyle=':')\n",
        "    plt.xlabel(\"Predictions\")\n",
        "    plt.ylabel(\"Residual\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Distribution of errors\n",
        "    p = sns.distplot(y_res_train,kde=True)\n",
        "    p = plt.title('Normality of error terms/residuals')\n",
        "    plt.xlabel(\"Residuals\")\n",
        "    plt.show()\n",
        "    \n",
        "    #### Variance\n",
        "    sns.regplot(x=y_train, y=y_pred_train)\n",
        "    plt.title('Predicted Points Vs. Actual Points', fontdict={'fontsize': 20})\n",
        "    plt.xlabel('Actual Points', fontdict={'fontsize': 15})\n",
        "    plt.ylabel('Predicted Points', fontdict={'fontsize': 15})\n",
        "    plt.show()\n",
        "    print(\"Shape after outlier correction \",price_df.shape ,\"rows & columns.\")\n",
        "\n",
        "def prediction_matrix(model,X_train,X_test,y_train,y_test):\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    metric = []\n",
        "    r2_train_lr = r2_score(y_train, y_pred_train)\n",
        "    rss_train_lr = np.sum(np.square(y_train - y_pred_train))\n",
        "    mse_train_lr = mean_squared_error(y_train, y_pred_train)\n",
        "    adjusted_r2_train_lr= (1 - (1-model.score(X_train, y_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n",
        "    \n",
        "    r2_test_lr = r2_score(y_test, y_pred_test)\n",
        "    rss_test_lr = np.sum(np.square(y_test - y_pred_test))\n",
        "    mse_test_lr = mean_squared_error(y_test, y_pred_test)\n",
        "    adjusted_r2_test_lr= (1 - (1-model.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)) \n",
        "    \n",
        "    \n",
        "    metric.append(r2_train_lr)\n",
        "    metric.append(r2_test_lr)\n",
        "    metric.append(adjusted_r2_train_lr)\n",
        "    metric.append(adjusted_r2_test_lr)\n",
        "    metric.append(rss_train_lr)\n",
        "    metric.append(mse_test_lr)\n",
        "    metric.append(mse_train_lr**0.5)\n",
        "    metric.append(mse_test_lr**0.5)\n",
        "\n",
        "    y_res_train = y_train - y_pred_train\n",
        "    y_res_test = y_test - y_pred_test\n",
        "    \n",
        "    plt.figure(figsize=(20, 12))\n",
        "    sns.set(font_scale= 1)\n",
        "    sns.set_style('whitegrid')\n",
        "    \n",
        "    plt.subplot(2,3,1)\n",
        "#    plt.scatter( y_pred_train , y_res_train)\n",
        "    sns.scatterplot(x=y_pred_test,y=y_res_test,color='Blue')\n",
        "    plt.axhline(y=0, color='r', linestyle=':')\n",
        "    plt.xlabel(\"Train Predictions\")\n",
        "    plt.ylabel(\"Train Residual\")\n",
        "    \n",
        "    plt.subplot(2,3,2)\n",
        "    p = sns.distplot(y_res_train,kde=True,color='Blue')\n",
        "    p = plt.title('Normality of error terms/residuals on Train Data')\n",
        "    plt.xlabel(\"Residuals\")\n",
        "    \n",
        "\n",
        "    plt.subplot(2,3,3)\n",
        "    p=sns.regplot(x=y_train, y=y_pred_train,color='Green')\n",
        "    p=plt.title('Predicted Points Vs. Actual Points on Train Data')\n",
        "    plt.xlabel('Actual Points')\n",
        "    plt.ylabel('Predicted Points')\n",
        "\n",
        "    plt.subplot(2,3,4)\n",
        "    sns.scatterplot(x=y_pred_test,y=y_res_test,color='Green')\n",
        "  #  plt.scatter(y_pred_test , y_res_test)\n",
        "    plt.axhline(y=0, color='r', linestyle=':')\n",
        "    plt.xlabel(\"Test Predictions\")\n",
        "    plt.ylabel(\"Test Residual\")\n",
        "\n",
        "    plt.subplot(2,3,5)\n",
        "    p = sns.distplot(y_res_test,kde=True,color='Green')\n",
        "    p = plt.title('Normality of error terms/residuals on Test Data')\n",
        "    plt.xlabel(\"Residuals\")\n",
        "\n",
        "    plt.subplot(2,3,6)\n",
        "    p=sns.regplot(x=y_test, y=y_pred_test,color='Green')\n",
        "    p=plt.title('Predicted Points Vs. Actual Points on Test Data')\n",
        "    plt.xlabel('Actual Points')\n",
        "    plt.ylabel('Predicted Points') \n",
        "    \n",
        "    sns.despine()\n",
        "    \n",
        "#    assumption_graph(y_train,y_pred_train)\n",
        "#    assumption_graph(y_test,y_pred_test)\n",
        "    return metric"
      ],
      "id": "biblical-manchester",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unlimited-bowling"
      },
      "source": [
        "def cleaning(df):\n",
        "\n",
        "    print(\"Records before dropping duplicates  : \" + str(df.shape[0]))\n",
        "    df.drop_duplicates(keep=False,inplace=True)\n",
        "    print(\"Records after dropping duplicates  : \" + str(df.shape[0]))    \n",
        "    \n",
        "    print(\"Null Value Analysis\")\n",
        "    price_metadata = getMetadata(df)\n",
        "    price_metadata_group = price_metadata.groupby(\"Null_Percentage\").count().reset_index()\n",
        "    price_metadata_group.sort_values([\"Null_Percentage\"], axis=0,ascending=False, inplace=True)\n",
        "    plot_pie_chart(price_metadata_group,\"Null_Percentage\",\"Null_Count\")\n",
        "\n",
        "    print('Completely Missing Data')\n",
        "    completly_missing_data = price_metadata[price_metadata[\"Null_Percentage\"] == 100.0]\n",
        "    drop_missing_column = completly_missing_data.index.tolist()\n",
        "    print(\"Null Columns before deleting  : \" + str(df.shape[1]))\n",
        "    df.drop(drop_missing_column,inplace=True,axis=1)\n",
        "    print(\"Null Columns after deleting : \" + str(df.shape[1]))\n",
        "    \n",
        "    print('80%+ Missing Data')\n",
        "    missing_data_greater_80 = price_metadata[(price_metadata[\"Null_Percentage\"] > 80.0) & \n",
        "                                         (price_metadata[\"Null_Percentage\"] < 100.0)]\n",
        "    drop_missing_column_80 = missing_data_greater_80.index.tolist()\n",
        "    #df.drop(drop_missing_column_80, axis =1, inplace=True)\n",
        "    display(drop_missing_column_80)\n",
        "    print(\"Shape after deleting unique value columns \",df.shape ,\"rows & columns.\")\n",
        "    \n",
        "    print('Identify and drop columns having single value as they will not add any value to our analysis')\n",
        "    unique_value = df.nunique()\n",
        "    col_with_only_one_value = unique_value[unique_value.values == 1]\n",
        "    col_to_drop = col_with_only_one_value.index.tolist()\n",
        "    display(col_to_drop)\n",
        "    df.drop(col_to_drop, axis =1, inplace=True)\n",
        "    print(\"Shape after deleting unique value columns \",df.shape ,\"rows & columns.\")\n",
        "    \n",
        "    print('Datatype Check')\n",
        "    price_data_type = getMetadata(df)\n",
        "    display(price_data_type[\"Datatype\"].value_counts())\n",
        "    \n",
        "    price_numeric = df.select_dtypes(include=['object'])\n",
        "    print('Object Records:')\n",
        "    display(price_numeric.head(5))\n",
        "\n",
        "    print('Non Object Records:')\n",
        "    price_object = df.select_dtypes(exclude=['object'])\n",
        "    display(price_object.head(5))\n",
        "\n",
        "    print('Columns requiring imputation:')\n",
        "    impute_columns = getMetadata(df)\n",
        "    impute_columns = impute_columns[impute_columns[\"Null_Count\"] > 0]\n",
        "    display(impute_columns.sort_values(by=\"Null_Count\",ascending = False))\n",
        "    \n",
        "    return df\n",
        "\n",
        "def outlier_treatment(df,columns):\n",
        "    for col in columns:\n",
        "        lower_limit=(outlier_info[(outlier_info.Columns==col)][\"Lower Limit\"].values)[0]\n",
        "        upper_limit=(outlier_info[(outlier_info.Columns==col)][\"Upper Limit\"].values)[0]\n",
        "        df[col]=np.where(df[col]>upper_limit,upper_limit,df[col])\n",
        "        df[col]=np.where(df[col]<lower_limit,lower_limit,df[col])\n",
        "    display(df[num_list].describe())\n",
        "\n",
        "    ### Post fixing outliers\n",
        "    outlier_columns=outlier_info[outlier_info.Outliers>0][\"Columns\"]\n",
        "    i=int(len(outlier_columns)/3)\n",
        "    plt.figure(figsize=(30,30))\n",
        "    sns.set(font_scale=1.0)\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    j=1\n",
        "    for p,c in enumerate(columns):\n",
        "        plt.subplot(i,i,j)\n",
        "        sns.boxplot(y=df[c],orient=\"h\")\n",
        "        plt.ylabel(c)\n",
        "        j=j+1\n",
        "    plt.show()\n",
        "    print(\"Shape after outlier correction \",df.shape ,\"rows & columns.\")\n",
        "    return df\n",
        "    \n",
        "def impute(df):\n",
        "    for name in df.select_dtypes(\"number\"):\n",
        "        df[name] = df[name].fillna(0)\n",
        "    for name in df.select_dtypes(\"object\"):\n",
        "        df[name] = df[name].fillna(\"None\")\n",
        "    return df\n",
        "\n",
        "def encode(df,nominal_feature,ordinal_feature):\n",
        "    numeric_feature=list(df.select_dtypes(exclude=['object']).columns)\n",
        "    display(len(ordinal_feature),len(nominal_feature),len(numeric_feature))\n",
        "    num_list=numeric_feature.copy()\n",
        "    price_ordinal=df[ordinal_feature]\n",
        "    price_nominal=df[nominal_feature]\n",
        "    price_numeric=df[num_list]\n",
        "    #### Label Encoding\n",
        "    df[ordinal_feature]=df[ordinal_feature].astype('category')\n",
        "    for catg in ordinal_feature:\n",
        "        df[catg]=df[catg].cat.codes\n",
        "    display(\"Shape after dummy encoding \",df.shape ,\"rows & columns.\")\n",
        "    ### One Hot Encoding\n",
        "    price_dummies = pd.get_dummies(price_nominal, drop_first=True)\n",
        "    df = df.drop(list(price_nominal.columns), axis=1)\n",
        "    df = pd.concat([price_dummies,df],axis = 1)\n",
        "    display(df.head())\n",
        "    print(\"Shape after dummy encoding \",df.shape ,\"rows & columns.\")\n",
        "    return df\n",
        "\n",
        "def baseline(X_train,y_train,X_test,y_test,models):\n",
        "    cross_metric_train = []\n",
        "    cross_metric_test =[]\n",
        "    for i,x in enumerate(models):\n",
        "        score=cross_val_score(x,X_train,y_train,cv=10,scoring='r2')\n",
        "        score2=cross_val_score(x,X_test,y_test,cv=10,scoring='r2')\n",
        "        cross_metric_train.append(score.mean())\n",
        "        cross_metric_test.append(score2.mean())\n",
        "    y=pd.Series(cross_metric_test,name='Test')\n",
        "    lr_table = {'Metric': ['LR','Lasso','Ridge'],\n",
        "            'Train': cross_metric_train\n",
        "            }\n",
        "    lr_df=pd.DataFrame(lr_table,columns=[\"Metric\",\"Train\"])\n",
        "    baseline_metric=pd.concat([lr_df,y],axis=1)\n",
        "    display(baseline_metric)\n",
        "\n",
        "\n",
        "def loadData():\n",
        "    #input_path='../input/house-prices-advanced-regression-techniques/train.csv'\n",
        "    train='../train.csv'\n",
        "    test='../test.csv'\n",
        "    train_df=pd.read_csv(train, index_col='Id')\n",
        "    test_df=pd.read_csv(test,index_col='Id')\n",
        "    display(train_df.shape,test_df.shape)\n",
        "    df=pd.concat([train_df,test_df],axis=0)\n",
        "    print('Top Five Records')\n",
        "    display(df.head())\n",
        "    print('Shape',df.info())\n",
        "    price_metadata=getMetadata(df)\n",
        "    display(price_metadata)   \n",
        "    cleaning(df)\n",
        "    \n",
        "    return df"
      ],
      "id": "unlimited-bowling",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "connected-forwarding"
      },
      "source": [
        "## Step 1.a: Load Data and Perform Data Cleaning "
      ],
      "id": "connected-forwarding"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "oriented-scheme",
        "outputId": "5846b591-5092-4ab8-a7af-226646813563"
      },
      "source": [
        "price_df=loadData()"
      ],
      "id": "oriented-scheme",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a276bc045d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprice_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-13ef0f7ab8e4>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y89fivd5Ur7s"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Y89fivd5Ur7s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crazy-lingerie"
      },
      "source": [
        "## Step 1.b: Imputation\n",
        "### 1.i: Imputing values"
      ],
      "id": "crazy-lingerie"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hispanic-vaccine"
      },
      "source": [
        "price_df[\"SalePrice\"].isnull().sum()"
      ],
      "id": "hispanic-vaccine",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equipped-lodging"
      },
      "source": [
        "price_df[\"MasVnrArea\"].fillna((price_df[\"MasVnrArea\"].median()),inplace=True)\n",
        "price_df[\"LotFrontage\"].fillna((price_df[\"LotFrontage\"].median()),inplace=True)\n",
        "#price_df[\"GarageYrBlt\"].fillna((price_df[\"YearBuilt\"]),inplace=True)\n",
        "price_df[\"PoolQC\"].fillna('NA',inplace=True)\n",
        "price_df[\"MiscFeature\"].fillna('NA',inplace=True)\n",
        "price_df[\"Alley\"].fillna('NA',inplace=True)\n",
        "price_df[\"Fence\"].fillna('NA',inplace=True)\n",
        "price_df[\"FireplaceQu\"].fillna('NA',inplace=True)\n",
        "price_df[\"GarageType\"].fillna('NA',inplace=True)\n",
        "price_df[\"GarageFinish\"].fillna('NA',inplace=True)\n",
        "price_df[\"GarageQual\"].fillna('NA',inplace=True)\n",
        "price_df[\"GarageCond\"].fillna('NA',inplace=True)\n",
        "price_df[\"BsmtExposure\"].fillna('NA',inplace=True)\n",
        "price_df[\"BsmtFinType2\"].fillna('NA',inplace=True)\n",
        "price_df[\"BsmtFinType1\"].fillna('NA',inplace=True)\n",
        "price_df[\"BsmtCond\"].fillna('NA',inplace=True)\n",
        "price_df[\"BsmtQual\"].fillna('NA',inplace=True)\n",
        "price_df[\"MasVnrType\"].fillna('None',inplace=True)\n",
        "price_df[\"Electrical\"].fillna((price_df[\"Electrical\"].mode()[0]),inplace=True)\n",
        "tmp2=price_df[['SalePrice','GarageYrBlt']]\n",
        "\n",
        "col_to_drop=['SalePrice','GarageYrBlt']\n",
        "price_df.drop(col_to_drop, axis=1, inplace=True)\n",
        "tmp1=impute(price_df)\n",
        "price_df=pd.concat([tmp1,tmp2],axis=1)\n",
        "\n",
        "impute_columns = getMetadata(price_df)\n",
        "impute_columns = impute_columns[impute_columns[\"Null_Count\"] > 0]\n",
        "impute_columns.sort_values(by=\"Null_Count\",ascending = False)"
      ],
      "id": "equipped-lodging",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sacred-testimony"
      },
      "source": [
        "#### Populating categorical values"
      ],
      "id": "sacred-testimony"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "placed-browser"
      },
      "source": [
        "price_df['MSSubClass'].replace({20:\"1-STORY 1946 & NEWER\",\n",
        "                               30:\"1-STORY 1945 & OLDER\",\n",
        "                               40:\"1-STORY W/FINISHED\",\n",
        "                               45:\"1-1/2 STORY - UNFINISHED\",\n",
        "                               50:\"1-1/2 STORY FINISHED\",\n",
        "                               60:\"2-STORY 1946 & NEWER\",\n",
        "                               70:\"2-STORY 1945 & OLDER\",\n",
        "                               75:\"2-1/2 STORY ALL AGES\",\n",
        "                               80:\"SPLIT OR MULTI-LEVEL\",\n",
        "                               85:\"SPLIT FOYER\",\n",
        "                               90:\"DUPLEX\",\n",
        "                               120:\"1-STORY PUD\",\n",
        "                               150:\"1-1/2 STORY PUD\",\n",
        "                               160:\"2-STORY PUD\",\n",
        "                               180:\"PUD - MULTILEVEL\",\n",
        "                               190:\"2 FAMILY CONVERSION\"                         \n",
        "                              },inplace=True)\n",
        "price_df['MoSold'].replace({1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",\n",
        "                         7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"}\n",
        "                        ,inplace=True)\n",
        "\n",
        "print(\"Shape after correcting categorical columns \",price_df.shape ,\"rows & columns.\")\n",
        "price_df.head(5)"
      ],
      "id": "placed-browser",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timely-arnold"
      },
      "source": [
        "## Step 1.c: Outlier Treatment"
      ],
      "id": "timely-arnold"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "assisted-function"
      },
      "source": [
        "cat_list,num_list=col_list(price_df)\n",
        "print(\"Category Columns:\",cat_list)\n",
        "print(\"Continous Columns:\",num_list)\n",
        "price_df[num_list].describe()\n",
        "outlier_info=outliers(price_df,num_list)\n",
        "display(outlier_info)\n",
        "outlier_columns_fix=outlier_info[(outlier_info[\"Upper Limit\"]>20) & (outlier_info.Outliers>0)][\"Columns\"]\n",
        "display(list(outlier_columns_fix))\n",
        "price_df=outlier_treatment(price_df,list(outlier_columns_fix))"
      ],
      "id": "assisted-function",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serial-equation"
      },
      "source": [
        "## Step 1.d: Feature Engineering"
      ],
      "id": "serial-equation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stainless-consultancy"
      },
      "source": [
        "price_df[\"Age\"]=price_df[\"YrSold\"]-price_df[\"YearBuilt\"]\n",
        "price_df[\"GarageAge\"]=price_df[\"YrSold\"] - price_df[\"GarageYrBlt\"]\n",
        "price_df[\"GarageAge\"].fillna(99,inplace=True)\n",
        "price_df[\"LivLotRatio\"] = price_df.GrLivArea / price_df.LotArea\n",
        "price_df[\"Spaciousness\"] = (price_df[\"1stFlrSF\"] + price_df[\"2ndFlrSF\"]) / price_df.TotRmsAbvGrd\n",
        "price_df[\"MedNhbdArea\"] = price_df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n",
        "price_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Story',''))\n",
        "price_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Fin',''))\n",
        "price_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Unf',''))\n",
        "price_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Lvl',''))\n",
        "price_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Foyer',''))\n",
        "price_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('S','1'))\n",
        "price_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].astype('float64')\n",
        "MSClass=[k for k,v in zip(list((price_df[\"MSSubClass\"].value_counts()).index),\n",
        "                          list((price_df[\"MSSubClass\"].value_counts()).values)) if v<70]\n",
        "Neig=[k for k,v in zip(list((price_df[\"Neighborhood\"].value_counts()).index),\n",
        "                       list((price_df[\"Neighborhood\"].value_counts()).values)) if v<50]\n",
        "price_df[\"MSSubClass\"]=price_df[\"MSSubClass\"].apply(lambda x: \"Others\" if x in MSClass else x)\n",
        "price_df[\"Neighborhood\"]=price_df[\"Neighborhood\"].apply(lambda x: \"Others\" if x in Neig else x)\n",
        "col_to_drop=[\"YrSold\",\"YearBuilt\",\"GarageYrBlt\",\"YearRemodAdd\"]\n",
        "price_df.drop(col_to_drop,inplace=True,axis=1)\n",
        "display(price_df.head())\n",
        "print(\"Shape after dervived columns \",price_df.shape ,\"rows & columns.\")\n",
        "x = price_df[\"SalePrice\"]\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.distplot(x)\n",
        "plt.show()\n",
        "price_df[\"SalePrice_log\"] = np.log(price_df.SalePrice)\n",
        "x = price_df.SalePrice_log\n",
        "sns.distplot(x)\n",
        "plt.show()"
      ],
      "id": "stainless-consultancy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "related-sister"
      },
      "source": [
        "## Step 2: Data Visualisation:"
      ],
      "id": "related-sister"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jewish-potter"
      },
      "source": [
        "impute_columns = getMetadata(price_df)\n",
        "impute_columns = impute_columns[impute_columns[\"Null_Count\"] > 0]\n",
        "impute_columns.sort_values(by=\"Null_Count\",ascending = False)"
      ],
      "id": "jewish-potter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ready-company"
      },
      "source": [
        "cat_list,num_list=col_list(price_df)\n",
        "print(\"Category Columns:\",cat_list)\n",
        "print(\"Continous Columns:\",num_list)\n",
        "display(len(cat_list),len(num_list))\n",
        "plt.figure(figsize = (25, 15))\n",
        "sns.heatmap(price_df[num_list].corr(), annot = True, cmap=\"YlGnBu\")\n",
        "plt.show()"
      ],
      "id": "ready-company",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buried-grammar"
      },
      "source": [
        "#### Visualising the continous columns columns\n",
        "Quality_features = ['LotFrontage', 'LotArea', 'MasVnrArea', 'TotalBsmtSF', 'GrLivArea',\n",
        "                    'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n",
        "                    'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n",
        "                    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal','Age','GarageAge']\n",
        "plt.figure(figsize=(20, 30))\n",
        "sns.set(font_scale= 1.2)\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "for i, feature in enumerate(Quality_features):\n",
        "    plt.subplot(7, 4, i+1)\n",
        "    sns.scatterplot(data=price_df, x=feature, y='SalePrice', palette=\"ch:.10\")         \n",
        "sns.despine()"
      ],
      "id": "buried-grammar",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spare-steps"
      },
      "source": [
        "# We will plot some joint histogram and scatter grphs to look at correlated features in more detail\n",
        "y = price_df[\"SalePrice\"]\n",
        "features = [\n",
        "    \"MasVnrArea\",\n",
        "    \"BsmtFinSF1\",\n",
        "    \"TotalBsmtSF\",\n",
        "    \"1stFlrSF\",\n",
        "    \"GrLivArea\",\n",
        "    \"FullBath\",\n",
        "    \"TotRmsAbvGrd\",\n",
        "    \"Fireplaces\",\n",
        "    \"GarageCars\",\n",
        "    \"GarageArea\",\n",
        "    \"LotArea\",\n",
        "    \"LotFrontage\",\n",
        "]\n",
        "\n",
        "for features in features:\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    x = price_df[features]\n",
        "    sns.jointplot(x=x, y=y, data=price_df)"
      ],
      "id": "spare-steps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sustainable-wichita"
      },
      "source": [
        "#### Visualising the categorical columns\n",
        "Quality_features = ['MSZoning','LandContour','Utilities','HouseStyle','OverallCond','RoofStyle','Exterior1st','ExterCond',\n",
        "                    'RoofMatl', 'ExterQual', 'BsmtQual', 'HeatingQC', 'CentralAir', \n",
        "                    'Electrical', 'KitchenQual', 'GarageQual','GarageType','SaleCondition','PoolQC','Alley','Fence']\n",
        "\n",
        "plt.figure(figsize=(30, 20))\n",
        "sns.set(font_scale= 1.2)\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "for i, feature in enumerate(Quality_features):\n",
        "    plt.subplot(6, 4, i+1)\n",
        "    sns.barplot(data=price_df, x=feature, y='SalePrice', palette=\"ch:.10\")         \n",
        "sns.despine()"
      ],
      "id": "sustainable-wichita",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dying-thesis"
      },
      "source": [
        "'''\n",
        "features = price_df.select_dtypes(include=['object']).columns\n",
        "plt.figure(figsize=(30, 20))\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "for feature in features:\n",
        "    g = sns.FacetGrid(price_df[~price_df.SalePrice.isnull()], col=feature)\n",
        "    g.map(plt.hist, 'SalePrice');\n",
        "    sns.despine()\n",
        "'''"
      ],
      "id": "dying-thesis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rational-giving"
      },
      "source": [
        "**Analysis** - Lots of categorical columns have data skewed to one/two category like:\n",
        "'MSZoning','Street','Alley','LandContour','Utilities','LandSlope',\n",
        "'Condition1','Condition2','BldgType','RoofStyle','RoofMatl','ExterCond','BsmtCond',\n",
        "'BsmtFinType2','Heating','CentralAir','Electrical','Functional',\n",
        "'GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition'\n",
        "and needs to binned properly"
      ],
      "id": "rational-giving"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tender-perfume"
      },
      "source": [
        "binn_col=['MSZoning','Street','Alley','LandContour',\n",
        "'Condition1','Condition2','BldgType','RoofStyle','RoofMatl','Heating','CentralAir','Electrical',\n",
        "'PavedDrive','Fence','MiscFeature','SaleType','SaleCondition']\n",
        "x=dict()\n",
        "x[\"MSZoning\"]=300\n",
        "x[\"Street\"]=10\n",
        "x[\"Alley\"]=60\n",
        "x[\"LandContour\"]=70\n",
        "x[\"LandSlope\"]=70\n",
        "x[\"Condition1\"]=100\n",
        "x[\"Condition2\"]=10\n",
        "x[\"BldgType\"]=120\n",
        "x[\"RoofStyle\"]=300\n",
        "x[\"RoofMatl\"]=15\n",
        "x[\"Heating\"]=20\n",
        "x['Electrical']=100\n",
        "x['Functional']=40\n",
        "x['PavedDrive']=100\n",
        "x['Fence']=160\n",
        "x['MiscFeature']=50\n",
        "x['SaleType']=130\n",
        "x['SaleCondition']=130\n",
        "for p,y in x.items():\n",
        "    val=[k for k,v in zip(list((price_df[p].value_counts()).index),\n",
        "                       list((price_df[p].value_counts()).values)) if v<y]\n",
        "    price_df[p]=price_df[p].apply(lambda x: \"Others\" if x in val else x)\n",
        "price_df.head()\n"
      ],
      "id": "tender-perfume",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "portuguese-lingerie"
      },
      "source": [
        "## Step 3: Data Prep"
      ],
      "id": "portuguese-lingerie"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "charged-sterling"
      },
      "source": [
        "ordinal_feature=['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC',\n",
        "                 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond','LotShape', 'LandSlope', 'BsmtExposure', \n",
        "                 'BsmtFinType1', 'BsmtFinType2', 'Functional','GarageFinish','Utilities','PoolQC'\n",
        "                 ]\n",
        "nominal_feature= [\"MSSubClass\", \"MSZoning\", \"Street\", \"LandContour\", \n",
        "                \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \n",
        "                 \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \n",
        "                \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \n",
        "                 \"SaleType\", \"SaleCondition\",\"PavedDrive\",'Electrical','MoSold','Alley','Fence','MiscFeature']\n",
        "df=encode(price_df,ordinal_feature,nominal_feature)"
      ],
      "id": "charged-sterling",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "electronic-services"
      },
      "source": [
        "## Step 4: Splitting and Scaling the data"
      ],
      "id": "electronic-services"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "canadian-spice"
      },
      "source": [
        "train=df[~df.SalePrice.isnull()]\n",
        "test=df[df.SalePrice.isnull()]\n",
        "display(train.shape,test.shape)"
      ],
      "id": "canadian-spice",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "latest-victorian"
      },
      "source": [
        "np.random.seed(0)\n",
        "df_train, df_validation = train_test_split(train, train_size = 0.7, test_size = 0.3, random_state = 100)"
      ],
      "id": "latest-victorian",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alert-currency"
      },
      "source": [
        "#### Dividing into X and Y sets for the model building"
      ],
      "id": "alert-currency"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "broad-liver"
      },
      "source": [
        "X_train = df_train.drop([\"SalePrice_log\",\"SalePrice\"], axis = 1)\n",
        "y_train = df_train[\"SalePrice_log\"]\n",
        "X_validation = df_validation.drop([\"SalePrice_log\",\"SalePrice\"], axis = 1)\n",
        "y_validation = df_validation[\"SalePrice_log\"]\n",
        "X_test = test.drop([\"SalePrice_log\",\"SalePrice\"], axis = 1)"
      ],
      "id": "broad-liver",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "demographic-account"
      },
      "source": [
        "num_list=list(X_train.select_dtypes(exclude=['object']).columns)"
      ],
      "id": "demographic-account",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "periodic-tulsa"
      },
      "source": [
        "num_list.remove('LivLotRatio')"
      ],
      "id": "periodic-tulsa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tracked-patrol"
      },
      "source": [
        "### Scaling\n",
        "scaler = RobustScaler()\n",
        "#scaler=StandardScaler()\n",
        "X_train[num_list] = scaler.fit_transform(X_train[num_list])\n",
        "X_validation[num_list] = scaler.transform(X_validation[num_list])\n",
        "X_train.head()"
      ],
      "id": "tracked-patrol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naked-matthew"
      },
      "source": [
        "X_test[num_list]=scaler.transform(X_test[num_list])"
      ],
      "id": "naked-matthew",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "familiar-excuse"
      },
      "source": [
        "## Step 5: Model Building & Evaluation"
      ],
      "id": "familiar-excuse"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surprised-marijuana"
      },
      "source": [
        "#### i. Baseline Score"
      ],
      "id": "surprised-marijuana"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stock-escape"
      },
      "source": [
        "models=[LinearRegression(),Lasso(),Ridge()]\n",
        "baseline(X_train,y_train,X_validation,y_validation,models)"
      ],
      "id": "stock-escape",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aggregate-liability"
      },
      "source": [
        "#### ii. Linear Regression"
      ],
      "id": "aggregate-liability"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cleared-clarity"
      },
      "source": [
        "lm = LinearRegression()\n",
        "lm.fit(X_train, y_train)\n",
        "lm_metric=prediction_matrix(lm,X_train,X_validation,y_train,y_validation)"
      ],
      "id": "cleared-clarity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "editorial-voice"
      },
      "source": [
        "#### iii. RFE Implementation for feature selection"
      ],
      "id": "editorial-voice"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auburn-leather"
      },
      "source": [
        "#cross_validation(X_train,y_train,lm)\n",
        "min_features_to_select = 1  # Minimum number of features to consider\n",
        "rfecv = RFECV(estimator=Ridge(), step=1, cv=10,\n",
        "              scoring='r2',\n",
        "              min_features_to_select=min_features_to_select)\n",
        "rfecv.fit(X_train, y_train)\n",
        "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
        "# Plot number of features VS. cross-validation scores\n",
        "plt.figure()\n",
        "plt.xlabel(\"Number of features selected\")\n",
        "plt.ylabel(\"Cross validation score\")\n",
        "plt.plot(range(min_features_to_select,\n",
        "               len(rfecv.grid_scores_) + min_features_to_select),\n",
        "         rfecv.grid_scores_)\n",
        "plt.show()"
      ],
      "id": "auburn-leather",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "handed-presence"
      },
      "source": [
        "### selecting columns post rfe\n",
        "col = X_train.columns[rfecv.support_]\n",
        "len(list(col))\n",
        "X_train_rfe = X_train[col]\n",
        "X_validation_rfe  = X_validation[col]\n",
        "lm_rfe=LinearRegression()\n",
        "lm_rfe.fit(X_train_rfe,y_train)\n",
        "lm_rfe_metric=prediction_matrix(lm_rfe,X_train_rfe,X_validation_rfe,y_train,y_validation)"
      ],
      "id": "handed-presence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foreign-cliff"
      },
      "source": [
        "#### iv. Ridge Regression"
      ],
      "id": "foreign-cliff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "square-diary"
      },
      "source": [
        "# list of alphas to tune - if value too high it will lead to underfitting, if it is too low, \n",
        "# it will not handle the overfitting\n",
        "params = {'alpha': [0.001, 0.01, 0.1, 1.0,10.0,20,50,100,150,200,500]}\n",
        "estimator = Ridge()\n",
        "# cross validation\n",
        "folds = 5\n",
        "model_cv = GridSearchCV(estimator = estimator, \n",
        "                        param_grid = params, \n",
        "                        scoring= 'neg_mean_absolute_error',  \n",
        "                        cv = folds, \n",
        "                        return_train_score=True,\n",
        "                        verbose = 1)            \n",
        "model_cv.fit(X_train_rfe, y_train) \n",
        "display(model_cv.best_params_)\n",
        "alpha = model_cv.best_params_['alpha']\n",
        "ridge = Ridge(alpha=alpha)\n",
        "ridge.fit(X_train_rfe, y_train)\n",
        "lm_ridge_metric=prediction_matrix(ridge,X_train_rfe,X_validation_rfe,y_train,y_validation)"
      ],
      "id": "square-diary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geographic-thought"
      },
      "source": [
        "#### v. Lasso Regression"
      ],
      "id": "geographic-thought"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sought-surrey"
      },
      "source": [
        "# list of alphas to tune - if value too high it will lead to underfitting, if it is too low, \n",
        "# it will not handle the overfitting\n",
        "params = {'alpha': [0.00001,0.0001,0.001, 0.01, 0.1, 1.0,10.0,20,50,100,150,200,500]}\n",
        "#params = {'alpha': [0.001, 0.0001, 0.0005, 0.005,0.003 ]}\n",
        "estimator = Lasso()\n",
        "# cross validation\n",
        "folds = 5\n",
        "model_cv = GridSearchCV(estimator = estimator, \n",
        "                        param_grid = params, \n",
        "                        scoring= 'neg_mean_absolute_error',  \n",
        "                        cv = folds, \n",
        "                        return_train_score=True,\n",
        "                        verbose = 1)            \n",
        "model_cv.fit(X_train, y_train) \n",
        "display(model_cv.best_params_)\n",
        "alpha = model_cv.best_params_['alpha']\n",
        "lasso = Lasso(alpha=alpha)\n",
        "lasso.fit(X_train, y_train) \n",
        "lm_lasso_metric=prediction_matrix(lasso,X_train,X_validation,y_train,y_validation)"
      ],
      "id": "sought-surrey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "magnetic-wisconsin"
      },
      "source": [
        "#### vi. Decision Tree"
      ],
      "id": "magnetic-wisconsin"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "approximate-macro"
      },
      "source": [
        "params = {\n",
        "    'criterion':['mse'],\n",
        "    'splitter':['best'],\n",
        "    'max_depth':[5,10,15,20,40,50],\n",
        "    'min_samples_split':[2,5,10,20,50,100],\n",
        "    'min_samples_leaf':[1,2,3,5,10,20],\n",
        "    'random_state':[42,100],\n",
        "}\n",
        "estimator = tree.DecisionTreeRegressor()\n",
        "# cross validation\n",
        "folds = 5\n",
        "model_cv = GridSearchCV(estimator = estimator, \n",
        "                        param_grid = params, \n",
        "                        scoring= 'neg_mean_absolute_error',  \n",
        "                        cv = folds, \n",
        "                        return_train_score=True,\n",
        "                        verbose = 1)  \n",
        "%%time\n",
        "model_cv.fit(X_train, y_train) \n",
        "display(model_cv.best_estimator_)\n",
        "lm_dt=model_cv.best_estimator_\n",
        "lm_dt.fit(X_train, y_train) \n",
        "lm_dt_metric=prediction_matrix(lm_dt,X_train,X_validation,y_train,y_validation)"
      ],
      "id": "approximate-macro",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "treated-engineering"
      },
      "source": [
        "#### vii. Random Forest"
      ],
      "id": "treated-engineering"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sustained-payday"
      },
      "source": [
        "params = {\n",
        "    'criterion':['mse'],\n",
        "    'max_depth':[5,10,15,20,40],\n",
        "    'min_samples_split':[5,10,20],\n",
        "    'min_samples_leaf':[3,5,10,20],\n",
        "    'random_state':[42,100],\n",
        "    'max_features': [50,75,100,140],\n",
        "    'n_estimators':[10,30,50,100],\n",
        "    'n_jobs':[-1],\n",
        "    'oob_score':[True]\n",
        "    \n",
        "}\n",
        "estimator = RandomForestRegressor()\n",
        "# cross validation\n",
        "folds = 3\n",
        "model_cv = GridSearchCV(estimator = estimator, \n",
        "                        param_grid = params, \n",
        "                        scoring= 'neg_mean_absolute_error',  \n",
        "                        cv = folds, \n",
        "                        return_train_score=True,\n",
        "                        verbose = 1)  \n",
        "model_cv.fit(X_train, y_train) \n",
        "display(model_cv.best_estimator_)\n",
        "lm_rfc=model_cv.best_estimator_\n",
        "lm_rfc.fit(X_train, y_train) \n",
        "lm_rfc_metric=prediction_matrix(lm_rfc,X_train,X_validation,y_train,y_validation)"
      ],
      "id": "sustained-payday",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mechanical-front"
      },
      "source": [
        "#### viii. KNN Regression"
      ],
      "id": "mechanical-front"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sought-chaos"
      },
      "source": [
        "lm_knn=KNeighborsRegressor(n_neighbors=5)\n",
        "lm_knn.fit(X_train, y_train) \n",
        "lm_knn_metric=prediction_matrix(lm_knn,X_train,X_validation,y_train,y_validation)"
      ],
      "id": "sought-chaos",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monthly-democracy"
      },
      "source": [
        "#### ix. Gradient Boosting"
      ],
      "id": "monthly-democracy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rural-palestine"
      },
      "source": [
        "lm_gbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=31)\n",
        "lm_gbr.fit(X_train, y_train) \n",
        "lm_gbr_metric=prediction_matrix(lm_gbr,X_train,X_validation,y_train,y_validation)"
      ],
      "id": "rural-palestine",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "valid-nowhere"
      },
      "source": [
        "#### x. XGB Regrssor"
      ],
      "id": "valid-nowhere"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reduced-cookie"
      },
      "source": [
        "# Creating a table which contain all the metrics\n",
        "lr_table = {'Metric': ['R2 Score (Train)','R2 Score (Test)','Adjusted R2 Score (Train)','Adjusted R2 Score (Test)','RSS (Train)','RSS (Test)',\n",
        "                       'RMSE (Train)','RMSE (Test)'], \n",
        "        'Linear Regression': lm_metric\n",
        "        }\n",
        "\n",
        "metric_lm = pd.DataFrame(lr_table ,columns = ['Metric', 'Linear Regression'] )\n",
        "metric_lm_rfe = pd.Series(lm_rfe_metric,name = 'RFE Linear Regression')\n",
        "metric_lm_ridge = pd.Series(lm_ridge_metric, name = 'Ridge Regression')\n",
        "metric_lm_lasso = pd.Series(lm_lasso_metric, name = 'Lasso Regression')\n",
        "metric_lm_dt = pd.Series(lm_dt_metric,name = 'Decision Tree Regression')\n",
        "metric_lm_rfc = pd.Series(lm_rfc_metric, name = 'Random Forest Regression')\n",
        "metric_lm_knn= pd.Series(lm_knn_metric, name = 'KNN Regression')\n",
        "metric_lm_gbr = pd.Series(lm_gbr_metric, name = 'Gradient Boosting Regression')\n",
        "\n",
        "final_metric = pd.concat([metric_lm,metric_lm_rfe,metric_lm_ridge,metric_lm_lasso,metric_lm_dt,metric_lm_rfc,metric_lm_knn,metric_lm_gbr], axis = 1)\n",
        "print(\"Model Peformance Metric:\")\n",
        "display(final_metric)"
      ],
      "id": "reduced-cookie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limiting-absence"
      },
      "source": [
        "## Step 6: Making Predictions Using the Final Model on the test data"
      ],
      "id": "limiting-absence"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "compound-characterization"
      },
      "source": [
        "X_test.shape"
      ],
      "id": "compound-characterization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weekly-being"
      },
      "source": [
        "temp=X_test\n",
        "temp=temp.reset_index()\n",
        "output_df=temp['Id']\n",
        "models=[lm,lasso,lm_dt,lm_rfc,lm_knn,lm_gbr]\n",
        "for i,m in enumerate(models):\n",
        "    filename=str(i)+'_submission_file.csv'\n",
        "    lm_price=m.predict(X_test)\n",
        "    mp = pd.Series(lm_price, name = 'SalePrice')\n",
        "    final_metric = pd.concat([output_df,mp], axis = 1)\n",
        "    final_metric['SalePrice']=final_metric['SalePrice'].apply(lambda x: np.exp(x))\n",
        "    final_metric.to_csv(filename,index =False)"
      ],
      "id": "weekly-being",
      "execution_count": null,
      "outputs": []
    }
  ]
}