{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confident-workshop",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "Naive Bayes is Supervised Classification Algorithm used for Text Classification like email is ham or spam.\n",
    "\n",
    "#### Conditional Probability and its intution:\n",
    "\n",
    "2 terms:\n",
    "\n",
    "1. Prior: Proability before knowing situation\n",
    "\n",
    "P(Play) = 20/30 days = 2/3  = 66%\n",
    "P(Spam) = 1/5 emails = 20%\n",
    "\n",
    "2. Posterior:Proability post knowing situation\n",
    "P(Play|Rain) = 10%\n",
    "P(Spam|Word=Viagara) = 70%\n",
    "\n",
    "\n",
    "$ P(A|B) = P(A,B)/P(B) $\n",
    "\n",
    "$ P(A and B) = P(A|B).P(B) $  ....1\n",
    "\n",
    "$ P(A and B) = P(B|A).P(A) $  ....2\n",
    "\n",
    "From eq 1 and 2\n",
    "\n",
    "$ P(B|A).P(A) = P(A|B).P(B) $\n",
    "\n",
    "$  P(B|A) = P(A|B).P(B) / P(A) $\n",
    "\n",
    "This is known as Bayes theorm\n",
    "\n",
    "#### To show that 2 variables are independent\n",
    "\n",
    "P(A and B) = P(A).P(B)\n",
    "P(A and B) = P(A|B). P(B) \n",
    "\n",
    "P(A|B).P(B) = P(A).P(B)\n",
    "\n",
    "\n",
    "#### Naive Bayes - With One Feature\n",
    "\n",
    "Naïve Bayes is a probabilistic classifier that returns the probability of a test point belonging to a class, using Bayes’ theorem. As you learned previously, Bayes’ theorem is defined as —\n",
    "\n",
    "$ P(C_{i}|x) = P(x|C_{i}). P(C_{i}) / P(x) $ \n",
    "\n",
    " denotes the classes, and X denotes the features of the data point.\n",
    "\n",
    "Probabilities are calculated simply by counting the number of instances/occurrences for categorical data.\n",
    "\n",
    "The effect of the denominator P(x) is not incorporated while calculating probabilities as it is the same for both the classes and hence, can be ignored without affecting the final outcome.\n",
    "\n",
    "The class assigned to the new test point is the class for which  $ P (C_{i}|x) $ is greater.\n",
    "\n",
    "##### Conditional Independence in Naive Bayes\n",
    "\n",
    "Naïve Bayes follows an assumption that the variables are conditionally independent given the class i.e. P(X = convex,smooth | C= edible) can be written as P(X=smooth | C=edible)\\timesP(X=convex | C=edible). The terms P(X=smooth | C=edible) and P(X=convex | C=edible) is simply calculated by counting the data points. Hence, the name “Naïve” because in most real-world situations the variables are not conditionally independent given the class label but most of the times the algorithm works nonetheless.\n",
    "\n",
    "Let us say you are trying to compute P(A and B | C). If P(A | C) is the same for all values of B and P(B | C) is the same for all values of A, then there is conditional independence between A and B, given C. This is when P(A and B | C) = P(A | C) x P(B | C), implying that A is not conditioned on B or vice versa.\n",
    "\n",
    "#### Naive Bayes - With Multiple Feature\n",
    "\n",
    "$ P(C_{i}|x,y) = P(x|C_{i}). P(y|C_{i}).P(C_{i}) / P(x) $ \n",
    "\n",
    "$ P(C= edible|smooth,convex)= P(smooth,convex|C=edible).P(C=edible) $\n",
    "\n",
    "$ P(C= edible|smooth,convex)= P(smooth|C=edible).P(convex|C=edible).P(C=edible) $\n",
    "\n",
    "$ P(C= posionous|smooth,convex)= P(smooth|C=posionous).P(convex|C=posionous).P(C=posionous) $\n",
    "\n",
    "\n",
    "#### Deciphering Naive Bayes\n",
    "\n",
    "$ P(C_{i}|x) = P(x|C_{i}). P(C_{i}) / P(x) $ \n",
    "\n",
    "$ P(X/C_{i}) $ is known as the prior probability. It is the probability of an event occurring before the collection of new data.  Prior plays an important role while classifying, when using Naïve Bayes, as it highly influences the class of the new test point.\n",
    "$ P(X/C_{i}) $ represents the likelihood function. It tells the likelihood of a data point occurring in a category. The conditional independence assumption is leveraged while computing the likelihood probability.\n",
    "The effect of the denominator P(x) is not incorporated while calculating probabilities as it is the same for both the classes and hence, can be ignored without affecting the final outcome.\n",
    "$ P(X/C_{i}) $ is called the posterior probability, which is finally compared for the classes, and the test point is assigned the class whose Posterior probability is greater.\n",
    "Prior, Posterior and Likelihood\n",
    "Let’s understand the terminology of Bayes theorem.\n",
    "\n",
    " \n",
    "\n",
    "You have been using 3 terms: P(Class = edible / poisonous), P(X | Class) and P(Class | X). Bayesian classification is based on the principle that ‘you combine your prior knowledge or beliefs about a population with the case specific information to get the actual (posterior) probability’.\n",
    "\n",
    "P(Class = edible) or P(Class = poisonous) is called the prior probability\n",
    "This incorporates our ‘prior beliefs’ before you collect specific information. If 90% of mushrooms are edible, then the prior probability is 0.90. Prior gets multiplied with the likelihood to give the posterior. In many cases, the prior has a tremendous effect on the classification. If the prior is neutral (50% are edible), then the likelihood may largely decide the outcome.\n",
    "\n",
    "P(X|Class) is the likelihood\n",
    "After agreeing upon the prior, you collect new, case-specific data (like plucking mushrooms randomly from a farm and observing the cap colours). Likelihood updates our prior beliefs with the new information. If you find a CONVEX mushroom, then you’d want to know how likely you were to find a convex one if you had only plucked edible mushrooms.\n",
    "\n",
    " \n",
    "\n",
    "If  P(CONVEX| edible) is high, say 80%, implying that there was an 80% chance of getting a convex mushroom if you only took from edible mushrooms, this will reflect in increased chances of the mushroom being edible.\n",
    "\n",
    " \n",
    "\n",
    "If the likelihood is neutral (e.g. 50%), then the prior probability may largely decide the outcome. If the prior is way too powerful, then likelihood often barely affects the result.\n",
    "\n",
    "P(Class = edible | X) is the posterior probability\n",
    "It is the outcome which combines prior beliefs and case-specific information. It is a balanced outcome of the prior and the likelihood.\n",
    "\n",
    "\n",
    "##### Introduction - Naive Bayes for Text Classification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
