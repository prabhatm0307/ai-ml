{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faced-synthesis",
   "metadata": {},
   "source": [
    "**Principal component analysis (PCA)** is one of the most commonly used dimensionality reduction techniques in the industry. By converting large data sets into smaller ones containing fewer variables, it helps in improving model performance, visualising complex data sets, and in many more areas.\n",
    "\n",
    "**Why PCA**\n",
    "\n",
    "A couple of situations where having a lot of features posed problems for us are as follows:\n",
    "\n",
    "The predictive model setup: Having a lot of correlated features lead to the multicollinearity problem. Iteratively removing features is time-consuming and also leads to some information loss.\n",
    "Data visualisation: It is not possible to visualise more than two variables at the same time using any 2-D plot. Therefore, finding relationships between the observations in a data set having several variables through visualisation is quite difficult. \n",
    "\n",
    "**Benefits of PCA**\n",
    "\n",
    "1. For data visualisation and EDA\n",
    "\n",
    "2. For creating uncorrelated features that can be input to a prediction model:  With a smaller number of uncorrelated features, the modelling process is faster and more stable as well.\n",
    "\n",
    "3. Finding latent themes in the data: If you have a data set containing the ratings given to different movies by Netflix users, PCA would be able to find latent themes like genre and, consequently, the ratings that users give to a particular genre.\n",
    "\n",
    "4. Noise reduction\n",
    "\n",
    "**What of PCA**\n",
    "PCA is fundamentally a dimensionality reduction technique; it helps in manipulating a data set to one with fewer variables. \n",
    "\n",
    "In simple terms, dimensionality reduction is the exercise of dropping the unnecessary variables, i.e., the ones that add no useful information. Now, this is something that you must have done in the previous modules. In EDA, you dropped columns that had a lot of nulls or duplicate values, and so on. In linear and logistic regression, you dropped columns based on their p-values and VIF scores in the feature elimination step.\n",
    "\n",
    "Similarly, what PCA does is that it converts the data by creating new features from old ones, where it becomes easier to decide which features to consider and which not to. \n",
    "\n",
    "PCA is a statistical procedure to convert observations of possibly correlated variables to ‘principal components’ such that:\n",
    "\n",
    "1. They are uncorrelated with each other.\n",
    "2. They are linear combinations of the original variables.\n",
    "3. They help in capturing maximum information in the data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-conditioning",
   "metadata": {},
   "source": [
    "**Basis**\n",
    "\n",
    "Basis is a unit in which we express the vectors of a matrix.\n",
    "For example, we describe the weight of an object in terms of the kilogram, gram, and so on; to describe length, we use a metre, centimetre, etc. So for example, when you say that an object has a length of 23 cm, what you are essentially saying is that the object’s length is 23×1 cm. Here, 1 cm is the unit in which you are expressing the length of the object.\n",
    "Similarly, vectors in any dimensional space or matrix can be represented as a linear combination of basis vectors. \n",
    "\n",
    "The basic definition of basis vectors is that they're a certain set of vectors whose linear combination is able to explain any other vector in that space.\n",
    "\n",
    "In vectors and vector spaces, we use basis vectors to represent the points in space. You understood how every observation in the space can be represented by scaling and adding the scaled basis vectors. This process is also called a linear combination.\n",
    "\n",
    " \n",
    "\n",
    "Then you learnt one of the key ideas that helped you connect basis vectors and the idea of dimensionality reduction: using different basis vectors to represent the same points.\n",
    "\n",
    " \n",
    "\n",
    "From there, you learnt how to change from one basis space to another using matrices. Here's a list of rules to help you revise the same.\n",
    "\n",
    "1.) If you're moving from a basis space Bto the standard basis, then the change of basis matrix M is the same as the basis vectors of B written as its column vectors. Therefore, if there is a vector v represented in B and you want to find its representation in the standard basis, then you'd have to perform Mv\n",
    " \n",
    "\n",
    "2.) If you want to go the other way around, where you have v represented in the standard basis and want to find its representation in B you multiply it by its inverse  - $ M^-1 v $\n",
    "\n",
    "\n",
    "3.) Finally, if you want to find the change of basis matrix M where you move from two non-standard basis vectors - say from B1 to B2 then you can get that by calculating this value - $ B_{2}^{-1}.B_{1} $\n",
    ". Note that in all the above cases, the basis vectors should be represented in the same units.\n",
    "\n",
    "**PCA and Change of Basis**\n",
    "\n",
    "1. PCA finds new basis vectors for us. These new basis vectors are also known as Principal Components.\n",
    "2. We represent the data using these new Principal Components by performing the change of basis calculations.\n",
    "3. After doing the change of basis, we can perform dimensionality reduction. In fact, PCA finds new basis vectors in such a way that it becomes easier for us to discard a few of the features.\n",
    "\n",
    "**Introduction to Variance**\n",
    "\n",
    "As mentioned previously, you have already learnt certain methods through which you delete columns – by checking the number of null values, unnecessary information and in modelling by checking the p-values and VIF scores.\n",
    "\n",
    " \n",
    "\n",
    "PCA gauges the importance of a column by another metric called ‘variance’ or how varied a column’s values are.\n",
    "\n",
    "**the importance of a column by checking its variance values. If a column has more variance, then this column will contain more information.**\n",
    "\n",
    "\n",
    "**Directions of Maximum Variance**\n",
    "Basically, the steps of PCA for finding the principal components can be summarised as follows.\n",
    "\n",
    "1. First, it finds the basis vector which is along the best- fit line that maximises the variance. This is our first principal component or PC1.\n",
    "2. The second principal component is perpendicular to the first principal component and contains the next highest amount of variance in the dataset.\n",
    "3. This process continues iteratively, i.e. each new principal component is perpendicular to all the previous principal components and should explain the next highest amount of variance.\n",
    "4. If the dataset contains n independent features, then PCA will create n Principal components.\n",
    "\n",
    "**The Workings of PCA**\n",
    "1. Find n new features - Choose a different set of n basis vectors (non-standard). These basis vectors are essentially the directions of maximum variance and are called Principal Components\n",
    "2. Express the original dataset using these new features\n",
    "3. Transform the dataset from the original basis to this PCA basis.\n",
    "4. Perform dimensionality reduction - Choose only a certain k (where k < n) number of the PCs to represent the data.  5. Remove those PCs which have fewer variance (explain less information) than others.\n",
    " \n",
    "\n",
    "PCA's role in the ML pipeline almost solely exists as a dimensionality reduction tool. Basically, you choose a fixed number of PCs that explained a certain threshold of variance that you have chosen and then uses only that many columns to represent the original dataset. This modified dataset is then passed on to the ML pipeline for further prediction algorithms to take place. PCA helps us in improving the model performance significantly and helps us in visualising higher-dimensional datasets as well.\n",
    "\n",
    "**Practical Considerations and Alternatives**\n",
    "\n",
    "1. Most software packages use SVD to compute the principal components and assume that the data is scaled and centred, so it is important to do standardisation/normalisation.\n",
    "2. PCA is a linear transformation method and works well in tandem with linear models such as linear regression, logistic regression, etc., though it can be used for computational efficiency with non-linear models as well.\n",
    "3. It should not be used forcefully to reduce dimensionality (when the features are not correlated).\n",
    "\n",
    "**Shortcoming**\n",
    "\n",
    "1. PCA is limited to linearity, though we can use non-linear techniques such as t-SNE as well\\\n",
    "2. PCA needs the components to be perpendicular, though in some cases, that may not be the best solution. The alternative technique is to use Independent Components Analysis. \n",
    "3. PCA assumes that columns with low variance are not useful, which might not be true in prediction setups (especially classification problem with a high class imbalance).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wired-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([[2,7,1],[-2,1,8],[3,4,-2]])\n",
    "b=np.array([[8,1,3],[3,5,8],[7,-2,-4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acquired-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 44,  35,  58],\n",
       "       [ 43, -13, -30],\n",
       "       [ 22,  27,  49]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "subsequent-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2],[2,-1]])\n",
    "b=np.array([[1,0],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "disturbed-fitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.],\n",
       "       [2.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=np.array([[3],[2]])\n",
    "B2=np.array([[3,-3],[4,-5]])\n",
    "B=np.linalg.inv(B2)\n",
    "B@A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "universal-power",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.],\n",
       "       [ 2., -1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "swiss-model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "30+20c+30d+5=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "30+20d=120\n",
    "d=3\n",
    "20c+5=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "endangered-maximum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>B3</th>\n",
       "      <th>B4</th>\n",
       "      <th>B5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   B1  B2  B3  B4  B5\n",
       "0   1   0   4   0   3\n",
       "1   2   3   4   3   2\n",
       "2   3   3   2   4   2\n",
       "3   4   4   3   5   4\n",
       "4   5   1   4   2   2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.read_csv(\"Ratings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "continuous-azerbaijan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(random_state=42)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=42)\n",
    "pca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "phantom-privacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.51886873e-01, 1.52836342e-01, 1.30717403e-01, 6.43810220e-02,\n",
       "       1.78359842e-04])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-suicide",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
